name: Test Suite

on:
  push:
    branches: [ "main", "master" ]
  pull_request:
    branches: [ "main", "master" ]

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  RUST_LOG: info
  DATABASE_URL: postgres://postgres:postgres@localhost:5432/acci_test
  CRITICAL_COVERAGE_THRESHOLD: 95
  CORE_COVERAGE_THRESHOLD: 90
  GENERAL_COVERAGE_THRESHOLD: 80

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: acci_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: llvm-tools-preview
          targets: wasm32-unknown-unknown

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2

      - name: Install required tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq bc
          cargo install cargo-llvm-cov

      - name: Run unit tests
        id: unit_tests
        run: |
          # Run tests and capture output
          make test-unit 2>&1 | tee test-output.log
          echo "status=${PIPESTATUS[0]}" >> "$GITHUB_OUTPUT"

      - name: Generate coverage reports
        if: always()
        run: |
          # Generate detailed coverage data
          cargo llvm-cov --lib --bins --all-features --workspace --exclude acci-tests \
            --lcov --output-path lcov.info \
            --html --output-dir coverage-html \
            --summary-only --output-path coverage-summary.txt \
            --json --output-path coverage.json

      - name: Check coverage thresholds
        if: always()
        run: |
          chmod +x scripts/check-coverage.sh
          ./scripts/check-coverage.sh coverage.json 2>&1 | tee coverage-check.log

      - name: Upload coverage to Coveralls
        if: always()
        uses: coverallsapp/github-action@v2
        with:
          file: lcov.info

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-artifacts
          path: |
            test-output.log
            coverage-html/
            coverage-summary.txt
            coverage.json
            coverage-check.log
            lcov.info

      - name: Process test results
        if: always()
        run: |
          if [[ "${{ steps.unit_tests.outputs.status }}" != "0" ]]; then
            echo "Unit tests failed. Check test-output.log for details."
            exit 1
          fi

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: acci_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2

      - name: Set up test environment
        run: |
          # Create logs directory
          mkdir -p test-logs
          # Set up environment variables for detailed logging
          echo "RUST_LOG=debug" >> $GITHUB_ENV
          echo "RUST_BACKTRACE=1" >> $GITHUB_ENV
          echo "TEST_LOG_DIR=$(pwd)/test-logs" >> $GITHUB_ENV

      - name: Run integration tests
        id: integration_tests
        run: |
          # Run tests with detailed output
          RUST_LOG=debug make test-integration 2>&1 | tee test-logs/integration-test-output.log
          echo "status=${PIPESTATUS[0]}" >> "$GITHUB_OUTPUT"

      - name: Collect container logs
        if: always()
        run: |
          # Collect Docker container logs
          docker ps -a > test-logs/docker-containers.log
          for container in $(docker ps -aq); do
            docker logs $container > test-logs/container-$container.log 2>&1
          done

      - name: Generate test report
        if: always()
        run: |
          # Generate a summary report
          {
            echo "# Integration Test Report"
            echo "## Test Execution Summary"
            echo "- Status: ${{ steps.integration_tests.outputs.status == '0' && 'Success' || 'Failure' }}"
            echo "- Timestamp: $(date -u)"
            echo "## Environment"
            echo "\`\`\`"
            env | grep -E "RUST_|TEST_|DATABASE_" | sort
            echo "\`\`\`"
            echo "## Test Output"
            echo "\`\`\`"
            tail -n 50 test-logs/integration-test-output.log
            echo "\`\`\`"
          } > test-logs/integration-test-report.md

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-artifacts
          path: |
            test-logs/
            target/debug/deps/*.d

      - name: Process test results
        if: always()
        run: |
          if [[ "${{ steps.integration_tests.outputs.status }}" != "0" ]]; then
            echo "Integration tests failed. Check test-logs/integration-test-output.log for details."
            exit 1
          fi

  mutation-testing:
    name: Mutation Testing
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    env:
      MUTATION_TIMEOUT: 600  # 10 minutes timeout per mutation
      MUTATION_THREADS: 4    # Number of parallel mutation threads
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better mutation analysis

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2
        with:
          key: mutation-testing

      - name: Install cargo-mutants
        uses: taiki-e/install-action@v2
        with:
          tool: cargo-mutants

      - name: Create mutation config
        run: |
          cat > .mutants.toml << EOF
          timeout = ${MUTATION_TIMEOUT}
          threads = ${MUTATION_THREADS}

          # Define mutation operators
          [[operators]]
          name = "arithmetic"
          enabled = true

          [[operators]]
          name = "conditional"
          enabled = true

          [[operators]]
          name = "function"
          enabled = true

          # Exclude test files and generated code
          exclude = [
            "**/tests/**",
            "**/generated/**",
            "**/migrations/**"
          ]
          EOF

      - name: Run mutation tests
        id: mutation_tests
        run: |
          # Create output directory
          mkdir -p mutation-results

          # Run mutation testing with parallel execution
          cargo mutants \
            --workspace \
            --all-features \
            --test-workspace true \
            --json > mutation-results/mutation-report.json \
            --html mutation-results/html \
            2>&1 | tee mutation-results/mutation-output.log

          # Extract and check mutation score
          score=$(jq '.score' mutation-results/mutation-report.json)
          echo "mutation_score=${score}" >> "$GITHUB_OUTPUT"

          if (( $(echo "$score < 80" | bc -l) )); then
            echo "::warning::Mutation score below threshold: $score% < 80%"
            echo "mutation_status=warning" >> "$GITHUB_OUTPUT"
          else
            echo "mutation_status=success" >> "$GITHUB_OUTPUT"
          fi

      - name: Generate mutation report
        if: always()
        run: |
          {
            echo "# Mutation Testing Report"
            echo "## Summary"
            echo "- Score: ${{ steps.mutation_tests.outputs.mutation_score }}%"
            echo "- Status: ${{ steps.mutation_tests.outputs.mutation_status }}"
            echo "- Timestamp: $(date -u)"
            echo
            echo "## Configuration"
            echo "\`\`\`toml"
            cat .mutants.toml
            echo "\`\`\`"
            echo
            echo "## Detailed Results"
            echo "See the HTML report in the artifacts for complete results."
            echo
            echo "### Failed Mutations"
            jq -r '.failed_mutations[] | "- " + .description' mutation-results/mutation-report.json || true
          } > mutation-results/mutation-report.md

      - name: Upload mutation artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mutation-test-artifacts
          path: |
            mutation-results/
            .mutants.toml

      - name: Process mutation results
        if: always()
        run: |
          if [[ "${{ steps.mutation_tests.outputs.mutation_status }}" == "warning" ]]; then
            echo "::warning::Mutation score is below threshold. Check mutation-results/mutation-report.md for details."
          fi

  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    env:
      BENCHMARK_DIR: target/criterion
      BENCHMARK_BASELINE_BRANCH: main
      PERFORMANCE_THRESHOLD: 10  # Maximum allowed performance regression (%)
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for baseline comparison

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2
        with:
          key: performance-testing

      - name: Set up benchmark environment
        run: |
          # Create benchmark directories
          mkdir -p $BENCHMARK_DIR/current
          mkdir -p $BENCHMARK_DIR/baseline

          # Install additional tools
          cargo install cargo-criterion || true
          cargo install hyperfine || true

      - name: Download baseline results
        continue-on-error: true
        run: |
          # Try to download baseline from artifacts
          gh run download --repo ${{ github.repository }} \
            --branch $BENCHMARK_BASELINE_BRANCH \
            --name benchmark-results || true

      - name: Run performance benchmarks
        id: benchmarks
        run: |
          # Run benchmarks and generate detailed reports
          cargo criterion \
            --message-format=json \
            --output-format=criterion \
            --plotting-backend=gnuplot \
            | tee benchmark-output.json

          # Run additional system benchmarks
          hyperfine --warmup 3 \
            'cargo run --release --example basic_operation' \
            --export-json system-benchmarks.json

          # Combine results
          jq -s '.[0] * .[1]' benchmark-output.json system-benchmarks.json > benchmark-results.json

      - name: Check for performance regressions
        id: regression_check
        run: |
          # Compare with baseline if available
          if [ -f "$BENCHMARK_DIR/baseline/benchmark-results.json" ]; then
            ./scripts/check-performance.sh \
              --current benchmark-results.json \
              --baseline $BENCHMARK_DIR/baseline/benchmark-results.json \
              --threshold $PERFORMANCE_THRESHOLD \
              --output regression-report.json

            # Extract regression status
            status=$(jq -r '.status' regression-report.json)
            echo "regression_status=${status}" >> "$GITHUB_OUTPUT"

            if [ "$status" = "regression" ]; then
              echo "::warning::Performance regression detected. Check regression-report.json for details."
            fi
          else
            echo "No baseline available for comparison"
            echo "regression_status=no_baseline" >> "$GITHUB_OUTPUT"
          fi

      - name: Generate performance report
        if: always()
        run: |
          {
            echo "# Performance Benchmark Report"
            echo "## Summary"
            echo "- Status: ${{ steps.regression_check.outputs.regression_status }}"
            echo "- Timestamp: $(date -u)"
            echo
            echo "## Configuration"
            echo "- Baseline Branch: $BENCHMARK_BASELINE_BRANCH"
            echo "- Regression Threshold: $PERFORMANCE_THRESHOLD%"
            echo
            echo "## Benchmark Results"
            if [ -f "regression-report.json" ]; then
              echo "### Regressions"
              jq -r '.regressions[] | "- " + .name + ": " + (.regression | tostring) + "% slower"' regression-report.json || true
              echo
              echo "### Improvements"
              jq -r '.improvements[] | "- " + .name + ": " + (.improvement | tostring) + "% faster"' regression-report.json || true
            else
              echo "No regression data available"
            fi
            echo
            echo "## System Benchmarks"
            jq -r '.results[] | "- " + .command + ": " + (.mean | tostring) + "s ±" + (.stddev | tostring) + "s"' system-benchmarks.json || true
          } > benchmark-report.md

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-artifacts
          path: |
            ${{ env.BENCHMARK_DIR }}/
            benchmark-results.json
            regression-report.json
            system-benchmarks.json
            benchmark-report.md

      - name: Process benchmark results
        if: always()
        run: |
          if [[ "${{ steps.regression_check.outputs.regression_status }}" == "regression" ]]; then
            echo "::warning::Performance regression detected. Check benchmark-report.md for details."
          fi

  metrics:
    name: Test Metrics
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, mutation-testing, performance]
    if: always()
    env:
      METRICS_DIR: test-metrics
      PROMETHEUS_PUSHGATEWAY: ${{ secrets.PROMETHEUS_PUSHGATEWAY }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up metrics environment
        run: |
          mkdir -p $METRICS_DIR
          # Install required tools
          sudo apt-get update
          sudo apt-get install -y jq bc

      - name: Download test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*-*artifacts"
          merge-multiple: true
          path: ${{ env.METRICS_DIR }}/artifacts

      - name: Process coverage metrics
        id: coverage
        if: always()
        run: |
          if [ -f "$METRICS_DIR/artifacts/unit-test-artifacts/coverage.json" ]; then
            coverage=$(jq -r '.total.coverage' "$METRICS_DIR/artifacts/unit-test-artifacts/coverage.json")
            echo "coverage=${coverage}" >> "$GITHUB_OUTPUT"

            if (( $(echo "$coverage < $GENERAL_COVERAGE_THRESHOLD" | bc -l) )); then
              echo "::warning::Coverage below threshold: ${coverage}% < ${GENERAL_COVERAGE_THRESHOLD}%"
              echo "coverage_status=warning" >> "$GITHUB_OUTPUT"
            else
              echo "coverage_status=success" >> "$GITHUB_OUTPUT"
            fi
          else
            echo "::warning::No coverage data found"
            echo "coverage_status=missing" >> "$GITHUB_OUTPUT"
          fi

      - name: Process mutation metrics
        id: mutation
        if: always()
        run: |
          if [ -f "$METRICS_DIR/artifacts/mutation-test-artifacts/mutation-results/mutation-report.json" ]; then
            score=$(jq -r '.score' "$METRICS_DIR/artifacts/mutation-test-artifacts/mutation-results/mutation-report.json")
            echo "mutation_score=${score}" >> "$GITHUB_OUTPUT"

            if (( $(echo "$score < 80" | bc -l) )); then
              echo "::warning::Mutation score below threshold: ${score}% < 80%"
              echo "mutation_status=warning" >> "$GITHUB_OUTPUT"
            else
              echo "mutation_status=success" >> "$GITHUB_OUTPUT"
            fi
          else
            echo "::warning::No mutation data found"
            echo "mutation_status=missing" >> "$GITHUB_OUTPUT"
          fi

      - name: Process performance metrics
        id: performance
        if: always()
        run: |
          if [ -f "$METRICS_DIR/artifacts/benchmark-artifacts/benchmark-results.json" ]; then
            # Extract performance metrics
            jq -r '.benchmarks[] | select(.mean != null) | "benchmark{\\"name\\"=\\"\(.name)\\"} \(.mean)"' \
              "$METRICS_DIR/artifacts/benchmark-artifacts/benchmark-results.json" > performance-metrics.txt

            if [ -f "$METRICS_DIR/artifacts/benchmark-artifacts/regression-report.json" ]; then
              status=$(jq -r '.status' "$METRICS_DIR/artifacts/benchmark-artifacts/regression-report.json")
              echo "performance_status=${status}" >> "$GITHUB_OUTPUT"

              if [ "$status" = "regression" ]; then
                echo "::warning::Performance regression detected"
              fi
            else
              echo "::warning::No regression data found"
              echo "performance_status=missing" >> "$GITHUB_OUTPUT"
            fi
          else
            echo "::warning::No performance data found"
            echo "performance_status=missing" >> "$GITHUB_OUTPUT"
          fi

      - name: Generate metrics report
        if: always()
        run: |
          {
            echo "# Test Metrics Report"
            echo "## Summary"
            echo "- Coverage: ${{ steps.coverage.outputs.coverage }}% (${{ steps.coverage.outputs.coverage_status }})"
            echo "- Mutation Score: ${{ steps.mutation.outputs.mutation_score }}% (${{ steps.mutation.outputs.mutation_status }})"
            echo "- Performance: ${{ steps.performance.outputs.performance_status }}"
            echo "- Timestamp: $(date -u)"
            echo
            echo "## Coverage Details"
            if [ -f "$METRICS_DIR/artifacts/unit-test-artifacts/coverage-summary.txt" ]; then
              echo "\`\`\`"
              cat "$METRICS_DIR/artifacts/unit-test-artifacts/coverage-summary.txt"
              echo "\`\`\`"
            else
              echo "No coverage details available"
            fi
            echo
            echo "## Mutation Testing Details"
            if [ -f "$METRICS_DIR/artifacts/mutation-test-artifacts/mutation-results/mutation-report.md" ]; then
              cat "$METRICS_DIR/artifacts/mutation-test-artifacts/mutation-results/mutation-report.md"
            else
              echo "No mutation testing details available"
            fi
            echo
            echo "## Performance Details"
            if [ -f "$METRICS_DIR/artifacts/benchmark-artifacts/benchmark-report.md" ]; then
              cat "$METRICS_DIR/artifacts/benchmark-artifacts/benchmark-report.md"
            else
              echo "No performance details available"
            fi
          } > "$METRICS_DIR/test-metrics-report.md"

      - name: Push metrics to Prometheus
        if: env.PROMETHEUS_PUSHGATEWAY != ''
        run: |
          # Prepare metrics for Prometheus
          {
            echo "# HELP test_coverage_percent Total test coverage percentage"
            echo "# TYPE test_coverage_percent gauge"
            echo "test_coverage_percent{branch=\"${{ github.ref }}\",commit=\"${{ github.sha }}\"} ${{ steps.coverage.outputs.coverage }}"

            echo "# HELP test_mutation_score_percent Mutation testing score percentage"
            echo "# TYPE test_mutation_score_percent gauge"
            echo "test_mutation_score_percent{branch=\"${{ github.ref }}\",commit=\"${{ github.sha }}\"} ${{ steps.mutation.outputs.mutation_score }}"

            if [ -f "performance-metrics.txt" ]; then
              echo "# HELP benchmark_duration_seconds Benchmark execution time in seconds"
              echo "# TYPE benchmark_duration_seconds gauge"
              cat performance-metrics.txt
            fi
          } > "$METRICS_DIR/prometheus-metrics.txt"

          # Push metrics to Prometheus Pushgateway
          cat "$METRICS_DIR/prometheus-metrics.txt" | curl --data-binary @- "$PROMETHEUS_PUSHGATEWAY/metrics/job/acci_tests/instance/${{ github.workflow }}"

      - name: Upload consolidated metrics
        uses: actions/upload-artifact@v4
        with:
          name: test-metrics-report
          path: |
            ${{ env.METRICS_DIR }}/test-metrics-report.md
            ${{ env.METRICS_DIR }}/prometheus-metrics.txt

      - name: Process overall status
        if: always()
        run: |
          if [[ "${{ steps.coverage.outputs.coverage_status }}" == "warning" ]] || \
             [[ "${{ steps.mutation.outputs.mutation_status }}" == "warning" ]] || \
             [[ "${{ steps.performance.outputs.performance_status }}" == "regression" ]]; then
            echo "::warning::One or more quality metrics are below thresholds. Check test-metrics-report.md for details."
          fi
